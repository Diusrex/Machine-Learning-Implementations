import random
import math

# Add base directory of project to path.
import os
import sys
dir_path = os.path.dirname(os.path.realpath(__file__))
sys.path.append(dir_path + "/../..")

from reinforcement_learning.explore_exploit.base import ExploreExploitBase

class UCB(ExploreExploitBase):
    """
    Follows optimism in the face of uncertainity, by estimating a maximum upper
    confidence bound for each that, with high probability, is compatible
    with the given observations.
    
    Has logarithmic expected regret, but does not reach the best problem-dependent
    best lower bound.
    
    Formula is: B = mean + c * sqrt(logN / (2t)), where c is a constant provided,
    t is the number of pulls for the current arm, and N is total number of pulls.
    
    Parameters
    --------
    actions : array-like, shape [n_actions,]
        All of the valid actions
    
    estimator : estimator.Estimator
        Should be able to estimate + update the expected reward for each of the different actions.
        For bandits, should use MeanEstimator.
        
    c
        Factor for the exploration factor in B estimate. As grows larger, will
        explore actions more.
        
    References
    --------
        Remi Munos. From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to
        Optimization and Planning. 2012.
    """
    def __init__(self, actions, estimator, c=math.sqrt(2)):
        super().__init__(actions, estimator)
        self._c = c
        
        
    def _on_reset(self):
        """
        Hook for child to handle resetting the class.
        
        Will be called in constructor.
        """
        self._T = 0
        
        self._t = [0 for _ in self._actions]
        # Reset with large number to ensure everything is selected once.
        self._inverse_sqrt_t = [100 for _ in self._actions]
        
        self._previous_ucb = [0 for _ in self._actions]
    
    def select_action(self):
        """
        Will return one of the possible actions.
        
        WARNING: update_from_action must be called after, otherwise may only select
        the same action.
        """
        # Is greedy on best action, since the estimate includes the UCB
        return self._get_best_action()
        
    
    def _on_update_from_action(self, action, r, estimates):
        """
        Hook for child to update any extra state/estimates as required.
        
        Parameters
        --------
        action_chosen : action
            Action chosen for this step.
        
        r : numeric
            Reward from choosing the current action.
        
        estimates : array-like, shape [n_actions,]
            The estimate for each action. Can be updated to change the overall
            estimates.
        """
        self._T += 1
        
        # Update the values for current action.
        action_index = self._action_to_index[action]
        self._t[action_index] += 1
        self._inverse_sqrt_t[action_index] = 1 / (2 * math.sqrt(self._t[action_index]))
        # The estimate for it was regenerated, so won't include the UCB calculation.
        self._previous_ucb[action_index] = 0
        
        sqrt_log_T = math.sqrt(math.log(self._T))
        
        for i in range(len(self._actions)):
            current_ucb = self._c * self._inverse_sqrt_t[i] * sqrt_log_T
            estimates[i] = estimates[i] - self._previous_ucb[i] + current_ucb
            self._previous_ucb[i] = current_ucb
        
    def describe(self):
        """
        Returns a formatted string to describe the implementation + parameters
        for this instance.
        """
        return "UBC {}".format(self._c)
